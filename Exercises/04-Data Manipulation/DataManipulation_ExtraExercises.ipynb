{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6rA13a4ZxCc"
      },
      "source": [
        "## Exercise 1: Import and Inspect Data\n",
        "\n",
        "- Import pandas and numpy.\n",
        "-     Load the \"tips\" dataset from seaborn (sns.load_dataset('tips')) and store it in a variable called tips.\n",
        "-     Print the first 5 rows of the dataset using the head() function.\n",
        "-    Display the summary statistics of the dataset using the describe() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JMp6RQPRZlFL"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_bill</th>\n",
              "      <th>tip</th>\n",
              "      <th>sex</th>\n",
              "      <th>smoker</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16.99</td>\n",
              "      <td>1.01</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.34</td>\n",
              "      <td>1.66</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21.01</td>\n",
              "      <td>3.50</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23.68</td>\n",
              "      <td>3.31</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24.59</td>\n",
              "      <td>3.61</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   total_bill   tip     sex smoker  day    time  size\n",
              "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
              "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
              "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
              "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
              "4       24.59  3.61  Female     No  Sun  Dinner     4"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "df=sns.load_dataset('tips')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgCw2uyhZ8TL"
      },
      "source": [
        "## Exercise 2: Filtering and Sorting\n",
        "\n",
        "-    Filter the dataset to only include rows where the total bill is greater than $20.\n",
        "-    Sort the filtered dataset by the tip amount in descending order.\n",
        "-    Reset the index of the sorted dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "AWhu-akWaGuU"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_bill</th>\n",
              "      <th>tip</th>\n",
              "      <th>sex</th>\n",
              "      <th>smoker</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50.81</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>48.33</td>\n",
              "      <td>9.00</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39.42</td>\n",
              "      <td>7.58</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48.27</td>\n",
              "      <td>6.73</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34.30</td>\n",
              "      <td>6.70</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Thur</td>\n",
              "      <td>Lunch</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>30.06</td>\n",
              "      <td>2.00</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>24.55</td>\n",
              "      <td>2.00</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Sun</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>22.67</td>\n",
              "      <td>2.00</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>26.41</td>\n",
              "      <td>1.50</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>32.83</td>\n",
              "      <td>1.17</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Sat</td>\n",
              "      <td>Dinner</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>97 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    total_bill    tip     sex smoker   day    time  size\n",
              "0        50.81  10.00    Male    Yes   Sat  Dinner     3\n",
              "1        48.33   9.00    Male     No   Sat  Dinner     4\n",
              "2        39.42   7.58    Male     No   Sat  Dinner     4\n",
              "3        48.27   6.73    Male     No   Sat  Dinner     4\n",
              "4        34.30   6.70    Male     No  Thur   Lunch     6\n",
              "..         ...    ...     ...    ...   ...     ...   ...\n",
              "92       30.06   2.00    Male    Yes   Sat  Dinner     3\n",
              "93       24.55   2.00    Male     No   Sun  Dinner     4\n",
              "94       22.67   2.00    Male    Yes   Sat  Dinner     2\n",
              "95       26.41   1.50  Female     No   Sat  Dinner     2\n",
              "96       32.83   1.17    Male    Yes   Sat  Dinner     2\n",
              "\n",
              "[97 rows x 7 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new=df[df['total_bill']>20].sort_values('tip',ascending=False).reset_index(drop=True)\n",
        "\n",
        "df_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3LV7X6aHJt"
      },
      "source": [
        "## Exercise 3: Grouping and Aggregation\n",
        "\n",
        "-    Group the dataset by the \"day\" column and calculate the average total bill for each day.\n",
        "-    Find the day with the highest average total bill."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "doNy2JTzaRhj"
      },
      "outputs": [],
      "source": [
        "aveg=df_new[['day','total_bill']].groupby('day').agg('mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_bill</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Sun</th>\n",
              "      <td>28.582162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     total_bill\n",
              "day            \n",
              "Sun   28.582162"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aveg[aveg['total_bill']==aveg['total_bill'].max()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF1yv2KWaR2b"
      },
      "source": [
        "## Exercise 4: Merging Data\n",
        "\n",
        "-   Load the \"flights\" dataset from seaborn (sns.load_dataset('flights')) and store it in a variable called flights.\n",
        "-    Filter the flights dataset to only include rows where the year is 1950.\n",
        "-    Merge the filtered flights dataset with the tips dataset on the \"month\" column (use the \"month\" column from flights and \"day\" column from tips). Note that you may need to rename the column in either dataset for merging.\n",
        "-    Calculate the average tip amount for each month in the merged dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJYLh-b7abCz"
      },
      "outputs": [],
      "source": [
        "flights=sns.load_dataset('flights')\n",
        "flights.head()\n",
        "\n",
        "flights[flights['year']==1950]\n",
        "\n",
        "df.replace({'Mon':'Jan',\n",
        "            'Tue':'Feb',\n",
        "            'Wed':'Mar',\n",
        "            'Thu':'Apr',\n",
        "            'Fri':'May',\n",
        "            'Sat':'Jun',\n",
        "            'Sun':'July'}, inplace=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>passengers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1949</td>\n",
              "      <td>Jan</td>\n",
              "      <td>112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1949</td>\n",
              "      <td>Feb</td>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1949</td>\n",
              "      <td>Mar</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1949</td>\n",
              "      <td>Apr</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1949</td>\n",
              "      <td>May</td>\n",
              "      <td>121</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year month  passengers\n",
              "0  1949   Jan         112\n",
              "1  1949   Feb         118\n",
              "2  1949   Mar         132\n",
              "3  1949   Apr         129\n",
              "4  1949   May         121"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flights.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'month'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m flights\u001b[39m.\u001b[39;49mmerge(df, on\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmonth\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mday\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      2\u001b[0m flights\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:10093\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10074\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m  10075\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m  10076\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10089\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m  10090\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m  10091\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge\u001b[39;00m \u001b[39mimport\u001b[39;00m merge\n\u001b[1;32m> 10093\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[0;32m  10094\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m  10095\u001b[0m         right,\n\u001b[0;32m  10096\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m  10097\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m  10098\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m  10099\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m  10100\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m  10101\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m  10102\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m  10103\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m  10104\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m  10105\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m  10106\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m  10107\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m--> 110\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    111\u001b[0m         left,\n\u001b[0;32m    112\u001b[0m         right,\n\u001b[0;32m    113\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m    114\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m    115\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m    116\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m    117\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m    118\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m    119\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    120\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m    121\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m    122\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:703\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cross \u001b[39m=\u001b[39m cross_col\n\u001b[0;32m    698\u001b[0m \u001b[39m# note this function has side effects\u001b[39;00m\n\u001b[0;32m    699\u001b[0m (\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[0;32m    701\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[0;32m    702\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[1;32m--> 703\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_merge_keys()\n\u001b[0;32m    705\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    706\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_coerce_merge_keys()\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1162\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1160\u001b[0m rk \u001b[39m=\u001b[39m cast(Hashable, rk)\n\u001b[0;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m rk \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1162\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39;49m_get_label_or_level_values(rk))\n\u001b[0;32m   1163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1164\u001b[0m     \u001b[39m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39mindex)\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:1850\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     values \u001b[39m=\u001b[39m (\n\u001b[0;32m   1845\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\n\u001b[0;32m   1846\u001b[0m         \u001b[39m.\u001b[39mget_level_values(key)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m         \u001b[39m.\u001b[39m_values\n\u001b[0;32m   1848\u001b[0m     )\n\u001b[0;32m   1849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1850\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1852\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
            "\u001b[1;31mKeyError\u001b[0m: 'month'"
          ]
        }
      ],
      "source": [
        "flights.concatanate()\n",
        "flights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhJf-accabbE"
      },
      "source": [
        "## Exercise 5: Handling Missing Data\n",
        "\n",
        "-    Introduce some missing values into the tips dataset by randomly replacing 5% of the \"total_bill\" values with NaN (use numpy.nan and numpy.random.choice()).\n",
        "-    Calculate the number of missing values in the \"total_bill\" column.\n",
        "-    Fill the missing values with the mean of the \"total_bill\" column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwOgKCV-al3a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc-L-LTOamGM"
      },
      "source": [
        "## Exercise 6: Data Transformation\n",
        "\n",
        "-    Create a new column \"tip_percentage\" in the tips dataset, which represents the tip amount as a percentage of the total bill.\n",
        "-    Create another new column \"tip_category\" by categorizing the \"tip_percentage\" column into \"low\", \"medium\", and \"high\" based on the 33rd and 66th percentiles of the \"tip_percentage\" column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0766Q05as7r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xopSUro8atQi"
      },
      "source": [
        "## Exercise 7: Time Series Analysis\n",
        "\n",
        "-    Load the \"fmri\" dataset from seaborn (sns.load_dataset('fmri')) and store it in a variable called fmri.\n",
        "-    Convert the \"timepoint\" column to a datetime object using pandas.to_datetime().\n",
        "-    Set the \"timepoint\" column as the index of the dataset.\n",
        "-    Resample the dataset to calculate the mean signal value for each subject at a 5-timepoint rolling window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBhaQjUDa1vm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pgr0ohAa1_j"
      },
      "source": [
        "## Exercise 8: Basic Statistical Analysis\n",
        "\n",
        "-    Calculate the correlation between the \"total_bill\" and \"tip\" columns in the tips dataset using numpy's numpy.corrcoef().\n",
        "-    Perform a t-test on the \"total_bill\" values for lunch and dinner in the tips dataset using scipy's scipy.stats.ttest_ind()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVp2bvj5a8K4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC0KZekza8uC"
      },
      "source": [
        "## Exercise 9: Multi-Index Manipulation\n",
        "\n",
        "-    Load the \"titanic\" dataset from seaborn (sns.load_dataset('titanic')) and store it in a variable called titanic.\n",
        "-    Group the dataset by the \"class\" and \"embark_town\" columns and calculate the average age for each group.\n",
        "-    Convert the resulting DataFrame into a multi-index format.\n",
        "- Use the .xs() function to extract the average age of passengers who embarked at \"Southampton\" across all classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5edjJblab3ot"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPV9eQTdb4xc"
      },
      "source": [
        "##     Exercise 10: Pivot Tables and Heatmaps\n",
        "\n",
        "-    Create a pivot table using the titanic dataset with \"class\" as the index, \"embark_town\" as the columns, and the average fare as the values.\n",
        "-    Use seaborn's sns.heatmap() function to visualize the pivot table as a heatmap. Customize the appearance of the heatmap (e.g., use a different colormap, add annotations, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYH4dlGpcFIg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roEc3w4rcFih"
      },
      "source": [
        "##     Exercise 11: Data Reshaping\n",
        "\n",
        "-    Load the \"iris\" dataset from seaborn (sns.load_dataset('iris')) and store it in a variable called iris.\n",
        "-    Reshape the iris dataset using pandas' melt() function to create a long-form dataset with columns \"variable\" (feature name), \"value\" (feature value), and \"species\".\n",
        "-    Create a boxplot using seaborn's sns.boxplot() function to visualize the distribution of each feature across the different iris species."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-4v4AyOcOvb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbGnSaqTcPIE"
      },
      "source": [
        "##     Exercise 12: Linear Regression and Model Evaluation\n",
        "\n",
        "-    Load the \"diamonds\" dataset from seaborn (sns.load_dataset('diamonds')) and store it in a variable called diamonds.\n",
        "-    Create a new column \"price_per_carat\" by dividing the \"price\" column by the \"carat\" column.\n",
        "-    Split the dataset into a training set (80%) and a testing set (20%).\n",
        "-    Fit a linear regression model using the training set to predict \"price_per_carat\" based on \"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\", and \"clarity\" (use pandas' get_dummies() function to convert the \"clarity\" column into dummy variables).\n",
        "-    Calculate the mean squared error (MSE) and R-squared score of the model on both the training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd8CeqJNcZY3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nsunYDwcaOx"
      },
      "source": [
        " ##   Exercise 13: Principal Component Analysis (PCA)\n",
        "\n",
        "-    Use the iris dataset from Exercise 11.\n",
        "-    Standardize the feature columns (i.e., \"sepal_length\", \"sepal_width\", \"petal_length\", and \"petal_width\") using sklearn's StandardScaler.\n",
        "-    Perform PCA using sklearn's PCA module to reduce the dimensionality of the standardized feature columns to 2 principal components.\n",
        "-    Create a scatter plot using seaborn's sns.scatterplot() function to visualize the two principal components, colored by the \"species\" column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr3uqdWtcg9X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqkbrHTRchRn"
      },
      "source": [
        "##    Exercise 14: K-Means Clustering\n",
        "\n",
        "-    Use the iris dataset from Exercise 11.\n",
        "-    Perform K-Means clustering on the standardized feature columns from - - Exercise 13 using sklearn's KMeans module with 3 clusters.\n",
        "-     Create a scatter plot using seaborn's sns.scatterplot() function to - visualize the two principal components from Exercise 13, colored by the assigned cluster labels.\n",
        "-     Compare the clustering results to the true iris species labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUXheqZCcxIz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqBwX_HPcxeM"
      },
      "source": [
        "##         Exercise 16: String Manipulation\n",
        "\n",
        "-    Load the \"titanic\" dataset from seaborn (sns.load_dataset('titanic')) and store it in a variable called titanic.\n",
        "-    Extract the title (e.g., Mr., Mrs., Miss., etc.) from the \"name\" column and store it in a new column called \"title\".\n",
        "-    Calculate the frequency of each title in the dataset.\n",
        "-    Replace rare titles with the string \"Other\" and update the \"title\" column.\n",
        "-    Create a bar plot using seaborn's sns.countplot() function to visualize the frequency of each title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV1W4uN2dUVO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFdfy7XCdUtz"
      },
      "source": [
        "##     Exercise 17: Date-Time Manipulation\n",
        "\n",
        "-    Create a pandas DataFrame with the columns \"date\" and \"value\", where \"date\" contains daily dates for the years 2020-2022, and \"value\" contains random integers between 1 and 100.\n",
        "-    Convert the \"date\" column to a datetime object using pandas.to_datetime().\n",
        "-    Set the \"date\" column as the index of the DataFrame.\n",
        "-    Calculate the monthly average of the \"value\" column.\n",
        "-    Calculate the rolling 7-day average of the \"value\" column.\n",
        "-    Create a line plot using seaborn's sns.lineplot() function to visualize the original \"value\" column, the monthly average, and the 7-day rolling average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwgGhbRGddp3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbj-lM-Zdd4f"
      },
      "source": [
        "##     Exercise 18: Regular Expressions\n",
        "\n",
        "-    Load the \"titanic\" dataset from seaborn (sns.load_dataset('titanic')) and store it in a variable called titanic.\n",
        "-    Use regular expressions to extract the cabin letter (e.g., 'A', 'B', 'C', etc.) from the \"cabin\" column and store it in a new column called \"cabin_letter\".\n",
        "-    Calculate the frequency of each cabin letter in the dataset.\n",
        "-    Create a bar plot using seaborn's sns.countplot() function to visualize the frequency of each cabin letter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM7yUidBdjOr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDs3gkU6djbK"
      },
      "source": [
        "##     Exercise 19: Time Zones and Daylight Saving Time\n",
        "\n",
        "-    Create a pandas DataFrame with the columns \"date\" and \"value\", where \"date\" contains hourly timestamps for the years 2020-2022, and \"value\" contains random integers between 1 and 100.\n",
        "-    Convert the \"date\" column to a datetime object using pandas.to_datetime().\n",
        "-    Set the \"date\" column as the index of the DataFrame.\n",
        "-    Localize the \"date\" index to the \"UTC\" time zone using the tz_localize() method.\n",
        "-    Convert the \"date\" index to the \"America/New_York\" time zone using the tz_convert() method.\n",
        "-    Determine the start and end dates of Daylight Saving Time for each year in the dataset.\n",
        "-    Calculate the average \"value\" for each hour of the day during Daylight Saving Time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_orS6b6OdqLD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkGmf30ydqcp"
      },
      "source": [
        "##     Exercise 20: Date Difference and Age Calculation\n",
        "\n",
        "-    Load the \"titanic\" dataset from seaborn (sns.load_dataset('titanic')) and store it in a variable called titanic.\n",
        "-    Assume the Titanic sank on April 15, 1912. Create a new column \"birthdate\" by subtracting the \"age\" column from the sinking date.\n",
        "-    Calculate the age in years of each passenger at the time of sinking by finding the difference between their \"birthdate\" and the sinking date, and then dividing by the number of days in a year (365.25).\n",
        "-    Compare the calculated age with the original \"age\" column and calculate the mean absolute error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OHKgkgUdv36"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5l84EwZdwLe"
      },
      "source": [
        "##     Exercise 21: Custom Date Formatting\n",
        "\n",
        "-    Create a pandas DataFrame with the columns \"date\" and \"value\", where \"date\" contains daily dates for the years 2020-2022 in the format 'YYYY-MM-DD', and \"value\" contains random integers between 1 and 100.\n",
        "-    Convert the \"date\" column to a datetime object using pandas.to_datetime() with the appropriate format string.\n",
        "-    Set the \"date\" column as the index of the DataFrame.\n",
        "-    Create a new column \"formatted_date\" containing the \"date\" column values formatted as 'Month DD, YYYY' (e.g., 'January 01, 2020') using the strftime() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL1HNVd2eGh3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9f0YxjeG6n"
      },
      "source": [
        "##     Exercise 22: Period and Frequency Conversion\n",
        "\n",
        " -   Create a pandas DataFrame with the columns \"date\" and \"value\", where \"date\" contains hourly timestamps for the years 2020-2022, and \"value\" contains random integers between 1 and 100.\n",
        "-    Convert the \"date\" column to a datetime object using pandas.to_datetime().\n",
        "-    Set the \"date\" column as the index of the DataFrame.\n",
        "-    Convert the DateTimeIndex to a PeriodIndex with a daily frequency using the to_period() method.\n",
        "-    Resample the DataFrame to a lower frequency (e.g., weekly) and calculate the sum of the \"value\" column for each new period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgksW7DoeM0D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pFklqm3eNKB"
      },
      "source": [
        "##     Exercise 23: Holidays and Business Days\n",
        "\n",
        "-    Create a pandas DataFrame with the columns \"date\" and \"value\", where \"date\" contains daily dates for the years 2020-2022, and \"value\" contains random integers between 1 and 100.\n",
        "-    Convert the \"date\" column to a datetime object using pandas.to_datetime().\n",
        "-    Set the \"date\" column as the index of the DataFrame.\n",
        "-    Create a new column \"is_holiday\" indicating whether the corresponding \"date\" is a US federal holiday using the pandas.tseries.holiday.USFederalHolidayCalendar() class.\n",
        "-    Create another new column \"is_business_day\" indicating whether the corresponding \"date\" is a business day using the pandas.offsets.BDay() class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob8-AoRdeSuE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5DgT8IpeS7G"
      },
      "source": [
        "##     Exercise 24: Lag and Lead Functions\n",
        "\n",
        "-    Load the \"flights\" dataset from seaborn (sns.load_dataset('flights')) and store it in a variable called flights.\n",
        "-    Convert the dataset into a time series format with the \"year\" and \"month\" columns combined into a single datetime column.\n",
        "-    Set the new datetime column as the index of the DataFrame.\n",
        "-    Calculate the 1-month lag and lead of the \"passengers\" column using the shift() method.\n",
        "-    Calculate the month-over-month growth rate of the \"passengers\" column by dividing the lead column by the lag column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwn5vp3teZSW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNAD7qBfeZmo"
      },
      "source": [
        "##     Exercise 25: Rolling Window Functions with Custom Functions\n",
        "\n",
        "-    Load the \"flights\" dataset from seaborn (sns.load_dataset('flights')) and store it in a variable called flights.\n",
        "-    Convert the dataset into a time series format with the \"year\" and \"month\" columns combined into a single datetime column.\n",
        "-    Set the new datetime column as the index of the DataFrame.\n",
        "-    Create a custom function to calculate the median absolute deviation (MAD) of a given series.\n",
        "-    Apply the custom MAD function to a rolling window of the \"passengers\" column with a window size of 12 months using the rolling() method and the apply() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJmFsbhleeyB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkvido7xefDe"
      },
      "source": [
        "##     Exercise 26: Load and Inspect NumPy Array\n",
        "\n",
        "-    Load the \"iris\" dataset from seaborn (sns.load_dataset('iris')) and store it in a variable called iris.\n",
        "-    Convert the iris DataFrame to a NumPy array (excluding the \"species\" column) using the values attribute.\n",
        "-    Print the shape and dtype of the NumPy array.\n",
        "-    Calculate the mean, median, and standard deviation of each feature in the array using NumPy functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXOUdKTOe8y_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk_ZD7IAe9LA"
      },
      "source": [
        "##     Exercise 27: Array Filtering and Sorting\n",
        "\n",
        "-    Use the iris NumPy array from Exercise 26.\n",
        "-    Filter the array to only include rows where the sepal length (first column) is greater than 5.5.\n",
        "-    Sort the filtered array by the petal length (third column) in ascending order using NumPy's argsort() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhaXsufvfCBV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuCoYWXNfCeE"
      },
      "source": [
        "##     Exercise 28: Array Broadcasting and Arithmetic\n",
        "\n",
        "-    Use the iris NumPy array from Exercise 26.\n",
        "-    Calculate the difference between the sepal length (first column) and sepal width (second column) for each row, and store the result in a new array.\n",
        "-    Calculate the sum of the petal length (third column) and petal width (fourth column) for each row, and store the result in another new array.\n",
        "-    Stack the two new arrays horizontally using NumPy's hstack() function to create a new 2D array with shape (150, 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDmoHgWlfIVa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7NRGeydfIt6"
      },
      "source": [
        "##     Exercise 29: Array Aggregation and Grouping\n",
        "\n",
        "-    Load the \"tips\" dataset from seaborn (sns.load_dataset('tips')) and store it in a variable called tips.\n",
        "-    Convert the \"total_bill\", \"tip\", and \"size\" columns of the tips DataFrame to a NumPy array using the values attribute.\n",
        "-    Create a dictionary to map each day of the week in the \"day\" column of the tips DataFrame to an integer (e.g., {'Thur': 0, 'Fri': 1, 'Sat': 2, 'Sun': 3}).\n",
        "-    Use the dictionary to create a NumPy array of integer labels corresponding to the \"day\" column of the tips DataFrame.\n",
        "-    Use NumPy's unique() function to find the unique integer labels.\n",
        "-    For each unique label, calculate the mean \"total_bill\", \"tip\", and \"size\" using NumPy's boolean indexing and aggregation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc1kCq1wfQCm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMRQKyP1fQhb"
      },
      "source": [
        "##     Exercise 30: Array Reshaping and Concatenation\n",
        "\n",
        "-    Load the \"flights\" dataset from seaborn (sns.load_dataset('flights')) and store it in a variable called flights.\n",
        "-    Convert the \"passengers\" column of the flights DataFrame to a NumPy array using the values attribute.\n",
        "-    Reshape the \"passengers\" array to a 2D array with shape (12, 12) where each row corresponds to a year (1949-1960) and each column corresponds to a month (January-December) using NumPy's reshape() function.\n",
        "-    Calculate the row-wise mean (average number of passengers per year) and column-wise mean (average number of passengers per month) using NumPy's mean() function with the axis parameter.\n",
        "-    Concatenate the row-wise mean and column-wise mean arrays into a single 2D array with shape (13, 13) using NumPy's concatenate() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy5VautzfY7W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKKcz9ZfZUR"
      },
      "source": [
        "##     Exercise 31: Scatter Plot with Regression Line\n",
        "\n",
        "-    Load the \"tips\" dataset from seaborn (sns.load_dataset('tips')) and store it in a variable called tips.\n",
        "-    Create a scatter plot using seaborn's sns.scatterplot() function to visualize the relationship between \"total_bill\" and \"tip\".\n",
        "-    Add a regression line to the scatter plot using seaborn's sns.regplot() function with the scatter=False parameter to show the linear relationship between \"total_bill\" and \"tip\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS3VtqcQfp46"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y-pDkDcfqK-"
      },
      "source": [
        "##     Exercise 32: Histogram and Kernel Density Estimate\n",
        "\n",
        "-    Load the \"diamonds\" dataset from seaborn (sns.load_dataset('diamonds')) and store it in a variable called diamonds.\n",
        "-    Create a histogram using seaborn's sns.histplot() function to visualize the distribution of the \"carat\" column.\n",
        "-    Add a kernel density estimate (KDE) to the histogram using seaborn's sns.kdeplot() function to show the estimated probability density function of the \"carat\" column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqxhULYJfvIm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hazQC935fvlp"
      },
      "source": [
        "##     Exercise 33: Box Plot and Violin Plot\n",
        "\n",
        "-    Load the \"titanic\" dataset from seaborn (sns.load_dataset('titanic')) and store it in a variable called titanic.\n",
        "-    Create a box plot using seaborn's sns.boxplot() function to visualize the distribution of the \"age\" column across different \"class\" categories.\n",
        "-    Create a violin plot using seaborn's sns.violinplot() function to visualize the distribution of the \"age\" column across different \"class\" categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMjjseRvf3xl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6V9DNHMf4Ci"
      },
      "source": [
        "##     Exercise 34: Pair Plot and Correlations\n",
        "\n",
        "-    Load the \"iris\" dataset from seaborn (sns.load_dataset('iris')) and store it in a variable called iris.\n",
        "-    Create a pair plot using seaborn's sns.pairplot() function to visualize the pairwise relationships between all feature columns in the iris dataset.\n",
        "-    Calculate the Pearson correlation coefficients between all feature columns using pandas' corr() method.\n",
        "-    Visualize the correlation matrix as a heatmap using seaborn's sns.heatmap() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOJ-VLrMf9oW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fenvOlHCf-fS"
      },
      "source": [
        "##     Exercise 35: Facet Grid with Multiple Plots\n",
        "\n",
        "-    Load the \"flights\" dataset from seaborn (sns.load_dataset('flights')) and store it in a variable called flights.\n",
        "-    Convert the \"passengers\" column to a z-score using the scipy.stats.zscore() function.\n",
        "-    Create a new column \"outlier\" in the flights DataFrame indicating whether the z-score of the \"passengers\" column is greater than 2 or less than -2.\n",
        "-    Create a FacetGrid using seaborn's sns.FacetGrid() function with the \"year\" as the row variable and \"outlier\" as the column variable.\n",
        "-    Map the sns.barplot() function to the FacetGrid to create bar plots of the \"month\" versus \"passengers\" for each facet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvQnyMUGgVxG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La_fy06ggWEX"
      },
      "source": [
        "##     Exercise 36: Regression Line with Confidence Interval\n",
        "\n",
        "-    Load the \"tips\" dataset from seaborn (sns.load_dataset('tips')) and store it in a variable called tips.\n",
        "-    Create a scatter plot using seaborn's sns.scatterplot() function to visualize the relationship between \"total_bill\" and \"tip\".\n",
        "-    Add a regression line with a 95% confidence interval to the scatter plot using seaborn's sns.regplot() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nglqy9FhgYxO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgxrXlx_gZDO"
      },
      "source": [
        "##     Exercise 37: ROC Curve and AUC Score\n",
        "\n",
        "-    Load the \"titanic\" dataset from seaborn (sns.load_dataset('titanic')) and store it in a variable called titanic.\n",
        "-    Prepare the dataset for a binary classification task by selecting relevant features and preprocessing the data.\n",
        "-    Split the dataset into a training set and a testing set using train_test_split() function from sklearn.model_selection.\n",
        "-    Train a logistic regression model using LogisticRegression class from sklearn.linear_model on the training set.\n",
        "-    Use the trained model to predict the probability of survival for the samples in the testing set.\n",
        "-    Calculate the true positive rate (TPR) and false positive rate (FPR) for various threshold values using roc_curve() function from sklearn.metrics.\n",
        "-    Calculate the area under the ROC curve (AUC) using roc_auc_score() function from sklearn.metrics.\n",
        "-    Plot the ROC curve (TPR vs FPR) using matplotlib's plt.plot() function and display the AUC score in the plot's title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd23EgNsgimj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLSTo1wugi8R"
      },
      "source": [
        "##     Exercise 38: Confusion Matrix and Classification Report\n",
        "\n",
        "-    Use the logistic regression model and predictions from Exercise 37.\n",
        "-    Calculate the confusion matrix using confusion_matrix() function from sklearn.metrics.\n",
        "-    Visualize the confusion matrix as a heatmap using seaborn's sns.heatmap() function with appropriate annotations.\n",
        "-    Calculate precision, recall, and F1-score for the binary classification task using classification_report() function from sklearn.metrics, and print the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSChKm9MgokW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W4Cnne3go6N"
      },
      "source": [
        "##     Exercise 39: Residuals Plot for Regression\n",
        "\n",
        "-    Load the \"diamonds\" dataset from seaborn (sns.load_dataset('diamonds')) and store it in a variable called diamonds.\n",
        "-    Prepare the dataset for a regression task by selecting relevant features and preprocessing the data.\n",
        "-    Split the dataset into a training set and a testing set using train_test_split() function from sklearn.model_selection.\n",
        "-    Train a linear regression model using LinearRegression class from sklearn.linear_model on the training set.\n",
        "-    Use the trained model to predict the price of diamonds in the testing set.\n",
        "-    Calculate the residuals (difference between the true price and the predicted price) for the samples in the testing set.\n",
        "-    Create a scatter plot using seaborn's sns.scatterplot() function to - visualize the relationship between the predicted price and the residuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKSeARn-gxw7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQxa8mF-gyaG"
      },
      "source": [
        "##     Exercise 40: Learning Curve for Model Evaluation\n",
        "\n",
        "-    Use the logistic regression model and titanic dataset from Exercise 37.\n",
        "-    Calculate the learning curve using the learning_curve() function from sklearn.model_selection with different training set sizes.\n",
        "-    Plot the learning curve (training set size vs model score) for both training and validation sets using matplotlib's plt.plot() function.\n",
        "-    Analyze the learning curve to assess the model's performance, potential overfitting or underfitting, and the impact of adding more training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XekdptTg3MR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BblZG_LlhLIG"
      },
      "source": [
        "##     Exercise 41: Comprehensive Exploratory Data Analysis\n",
        "\n",
        "-    Load the \"houseprices\" dataset from seaborn (sns.load_dataset('houseprices')) and store it in a variable called houseprices.\n",
        "-    Inspect the dataset using pandas methods such as info(), describe(), and head() to get an overview of the dataset's structure and features.\n",
        "-    Perform data cleaning and preprocessing by handling missing values, outliers, and data transformation as necessary.\n",
        "-    Conduct univariate analysis for both continuous and categorical features:\n",
        "        - For continuous features, create histograms and box plots using seaborn's sns.histplot() and sns.boxplot() functions.\n",
        "        - For categorical features, create bar plots and count plots using seaborn's sns.barplot() and sns.countplot() functions.\n",
        "-    Conduct bivariate analysis to investigate relationships between the target variable (e.g., sale price) and each feature:\n",
        "        - For continuous features, create scatter plots and calculate correlation coefficients using seaborn's sns.scatterplot() function and pandas' corr() method.\n",
        "        - For categorical features, create box plots and violin plots to visualize the distribution of the target variable across different categories using seaborn's sns.boxplot() and sns.violinplot() functions.\n",
        "-    Conduct multivariate analysis to investigate relationships between multiple features and the target variable:\n",
        "        - Create a correlation matrix heatmap using seaborn's sns.heatmap() function to visualize the relationships between all continuous features.\n",
        "        - Create pair plots and scatter plot matrices using seaborn's sns.pairplot() function to visualize pairwise relationships between a subset of important continuous features and the target variable.\n",
        "        - Create facet grids and conditioned plots using seaborn's sns.FacetGrid() function to visualize the relationships between a subset of important categorical features, continuous features, and the target variable.\n",
        "-    Summarize your findings from the EDA and provide recommendations for feature selection and further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOXiPbyGhKoL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
